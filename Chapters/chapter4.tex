 
This work was first discussed as a possibility in  \citep{Schneider2014}  and 
forms the basis of the analyses performed in Schneider et al. (in prep.).

\section{Introduction} 
 
Conventional approach is to use the two-point correlation function (2PCF).




We need a flexible, generative regression model 
that can specify the joint distribution of both the input and the output.
Simultaneously capture the rich  physical relationship between the different
lensing observables.  


In this work, we 
1) illustrate the basic properties of the Gaussian Process and how it is
similar or different from other approaches for cosmic shear inference  \\ 
2) show the derivation for incorporating the lensing physics in 
a suitable covariance kernel form for a Gaussian Process  \\
3) lay out the statistical model for making a mass map  
with the Gaussian Process with the modified kernel \\ 
4) discuss possible extensions and future directions 
 
Our method provides an alternative from having to use the 2PCF to represent the
Gaussian perturbations and other higher order
statistics for representing the non-Gaussian part of the cosmic shear information. 

Source galaxies provide sparse constraints to probe the matter density along
the line of sight. 

\section{Method}
\subsection{the basics of a Gaussian Process}
A Gaussian Process is one of the most highly published and studied 
generative model. It allows the inference of the joint probability density 
function of both the inputs and the outputs. 
This gives the model the flexibility for a wide range
of successful applications in many fields. 
It can be used for as a model for the light curve of
exoplanets \citep{Ambikasaran2014a}, 
or as a prior probability for optimizing the hyperparameters of
neural networks \citep{Snoek2012}, 
or be used for other classification and pattern extraction tasks 
\citep{Rasmussen2006}.

It is helpful to understand the mathematical formulation of a GP 
before discussing how we can adapt the GP to model the lensing observables. 
In a nutshell, a GP smooths the input data using a non-parametric kernel
\citep{Hastie1990}. 
It is the generalization of a multivariate Gaussian 
to infinite dimension \citep{Rasmussen2006}. 
While a multivariate Gaussian is parametrized by  
the mean vector $\mathbf{\mu}$ and the covariance matrix $\Sigma$, 
a GP is specified by a mean vector function $m(\xv)$ and a
covariance kernel function $\kerngp(\xv, \yv)$, based on some input spatial or
temporal coordinates $\xv$. In our case, the vector $\xv$ (and the
interchangeable notation $\yv$) denotes the
two-dimensional spatial coordinates of $\ngal$ source galaxy locations. 
The drawn collections of values $\psi(\xv)$ from a GP carry the correlation structures
specified by the kernel $\kerngp(\xv, \yv)$, and can be thought of as
functions: 
\begin{equation}
		[\psi_1(\xv), \psi_2(\xv) \ldots, \psi_m(\xv) ]^T \sim \gp(m(\xv), \kerngp(\xv, \xv')).
		\label{eq:GP}
\end{equation}
% Specifically, the mean vector $m(x)$ is often set to be zero in the
% GP. The data are usually mean-subtracted before fitting a GP.  
This shows the probabilistic nature of the GP predictions.
(See Fig. [TODO])
Each drawn function represents one realization of the smoothed
field, which will be used to model the lensing potential $\psi$ later on. 
By drawing many realizations of $\psi$, we can obtain the
corresponding credible levels.
Note that the credible levels are wider for regions with less data to reflect
higher uncertainty. A GP is said to have indefinite dimension due to its ability to
predict values in unobserved regions, even though the 
observed part of the kernel is of finite size (e.g. $\ngal \times \ngal$). 
By averaging the drawn realizations, we can
obtain the mean prediction. The mean predictions from a GP are
often more well behaved and do not show drastic changes 
in the tail regions where there is no data as other polynomial interpolation schemes 
would.  

% \caption{A figure denoting a GP. The gray region represents the possible
% realizations of the GP while the red curve represents the mean prediction. 
% The width of the credible regions reflects how uncertain the predictions are,
% the less data constraints we have, the larger the uncertainty. 
% }
When the input data is mean-subtracted, we can use a zero mean function in the
GP and the covariance kernel completely specifies the GP. 
There are several families of commonly used covariance kernel.
The ones that are of the highest interest are kernels that can capture 
the homogeneity and isotropy of the data, so the predicted fields can   
be consistent with the large-scale statistical properties of 
matter perturbations.
On the other hand, there are other types of periodic kernels that may be useful
in modeling systematics such as masking or footprints of chip gaps due to the
stacking of multiple exposures.
In reality, composite kernels formed by the addition and/or multiplication of
kernels are used to pick up (sometimes non-obvious) patterns in the data. 
A GP with composite kernels is a
famous example how both the short and long term trends of global carbon
dioxide level can be modeled and predicted [TODO cite].

\subsection{Adapting the exponential squared kernel with lensing physics}
One of the most popular kernels that can generate homogeneous and
isotropic data is the exponential squared kernel: 
\begin{equation}
	\kerngp(r^2) = \lambda^{-1} \exp\left(\frac{-r^2}{2 l^2}\right),
	\label{eq:exp_sq_kernel}
\end{equation}
where 
\begin{equation}
	r^2 \equiv (\xv - \yv)^T \Matrix{D} (\xv - \yv), 
\end{equation}
which only depends on the
squared magnitude of distances between pairs of galaxy locations. 
This chosen form is therefore invariant
under translational and rotational transformations.
The metric $\Matrix{D}$ is taken to be an identity matrix but  
can be generalized to account for anamorphic distortions. 
The precision parameter $\lambda$ affects the 
amplitude of the density perturbation, while the correlation length $l^2$ 
determines how fast the correlations between different spatial locations of the
field $\psi$ fall off as a function of distance.

Additionally, the exponential squared kernel is infinitely differentiable. This
differentiable kernel choice allows us to derive an analytical expression to
relate the different lensing observables.  
After using the Born approximation to express the lensing potential $\psi$,
each of the lensed observables, $\lensparams(\xv) \equiv [\kappa, \gamma_1,
\gamma_2]$ is related to the lensing potential $\psi$ via derivatives in the form of:
\begin{align}
\kappa &= \frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1^2} +
\frac{\partial^2 \psi}{\partial x_2^2 }\right) 
= \frac{1}{2} (\psi_{,11} + \psi_{,22}),\\ 
\gamma_1 
&=\frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1^2} - 
\frac{\partial^2 \psi}{\partial x_2^2}\right) 
= \frac{1}{2} (\psi_{,11} - \psi_{,22}), \\
\gamma_2 
&=\frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1 \partial
x_2} + \frac{\partial^2 \psi}{\partial x_2 \partial x_1}\right)
= \frac{1}{2} (\psi_{,12} + \psi_{,21}), 
\end{align}
where we have defined the shorthand for spatial derivatives with
subscripts for $h,i,j,k = 1, 2$ after a comma.
Since the covariance operator is linear,
the derivatives for the covariance kernels of interests can be shown to be:
\begin{align}
	\label{eq:kernel_derivatives1}
	\Cov (\kappa(\xv), \kappa(\yv))
&= \frac{1}{4}\left(
\kerngp_{,1111} + \kerngp_{,1122} + \kerngp_{,2211} + \kerngp_{,2222}
\right), \\
\Cov(\kappa(\xv), \gamma_1(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1111} + \kerngp_{,2211} - \kerngp_{,1122} - \kerngp_{,2222}
\right), \\
\Cov(\kappa(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1112} + \kerngp_{,2212} + \kerngp_{,1121} + \kerngp_{,2221}
\right),\\
\Cov(\gamma_1(\xv), \gamma_1(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1111} - \kerngp_{,1122} - \kerngp_{,2211} + \kerngp_{,2222}
\right), \\
\Cov(\gamma_1(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1112} + \kerngp_{,1121} - \kerngp_{,2212} - \kerngp_{,2221}
\right), \\
\Cov(\gamma_2(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1212} + \kerngp_{,1221} + \kerngp_{,2112} + \kerngp_{,2121}
\right),
	% \label{eq:kernel_derivatives2}
\end{align}
where
\begin{equation}
	\kerngp_{,hijk} = \frac{\partial^4 \kerngp}{\partial x_h \partial x_i
	\partial y_j \partial y_k}.
\end{equation}

With the definition of ${\bf \chi}_i$ as follows:
\begin{equation}
	\frac{\partial \Matrix{S}^2}{\partial \xv_i} = -\frac{\partial
	\Matrix{S}^2}{\partial \yv_i} =
	2 \Matrix{D}(\xv - \yv)_i \equiv 2{\bf \chi}_i,
\end{equation}
we can show that each entry $\nu_{,hijk}$ of $\kerngp_{,hijk}$ is
related to each entry $\nu$ of the original exponential squared kernel
$\kerngp$ by:
\begin{equation}
\nu_{,x_h x_i y_j y_k} = (\beta^4 \chi_h \chi_i \chi_j \chi_k -
\beta^3 (\chi_h \chi_i \Matrix{D}_{jk} \delta_{jk} + 5~{\rm perm.}) + \beta^2
(\Matrix{D}_{hj} \Matrix{D}_{ik}\delta_{hj}\delta_{ik} + 2~{\rm perm.})) \nu,
\label{eq:4thderivatives}
\end{equation}
with $\beta = l^{-2}$. There are 6 permutations (abbreviated as perm.) of the terms in
\ref{eq:4thderivatives}
multiplied to $\beta^3$ due
to choosing two pairs of indices from the $h,i,j,k$ where the order of each
pair of indices matters. Likewise, there are 3 possible permutations of
the terms the multiplied to $\beta^2$ due to choosing two pairs of indices from
$h, i, j, k$ where the order of the pairs does not matter.
The derivation of the relevant derivatives and terms are available 
in the appendix for completeness. 

\section{Preliminary results}
Our work is the first attempt to represent the information in cosmic shear
without assuming a Gaussian distribution for the density modes.  

We can simultaneously draw the  

% Is a non-trivial optimization problem 

\section{Discussion}

\subsection{Relating the lensing observables to a cosmological model}
The constraints on cosmological parameters need to be verified in a mock
weak-lensing analysis of cosmological simulation. 
It can enable the separation of E and B mode in the convergence field as shown
in Schneider et al. (in prep.) 

\subsection{Use of the Gaussian Process in the model fitting for shear}
We describe a simplified graphical model to demonstrate how the derived GP can
be used to infer cosmic shear and a mass map from the convergence.
A more sophisticated model can be found in Schneider et al. 2016.



% - [ ] add PGM 
% talk about the lensing approximations that were made 


% \subsection{Sampling grid and the correlation length}
% A weakness of the method is the apparent reliance on having the sampling 


\subsection{Separation of the E / B mode}
Under the same formalism, it is also possible to rewrite our covariance kernel 
to separate the E and the B mode. 

\subsection{Modeling systematics and noise}
% Can handle sharp boundaries from stacking images 
% cite Jee2013a  
% cite Rowe2010 paper for interpolating star fields for 
% PSF ellipticity corrections 

\subsection{Alternative kernel choice(s) for encoding the lensing physics}
Alternative choices for a covariance kernel for describing the lensing physics 
include the family of Mat\'{e}rn kernels.
[TODO add form of expression] 
The smoothness of the data generated from a Mat\'{e}rn kernel is closely
related to the degree of freedom of the kernel. 
The higher degree of freedom it has, the smoother the spatial variations of the
drawn $\psi$ would be for the same correlation length.
Note that a Mat\'{e}rn kernel is also only differentiable to the $i$-th order,
where $i$ is [TODO] related to the degree of freedom by [TODO].
This restricts us to use kernels with degree of freedom above [TODO]. 
When we take the limit of the degree of freedom of a Mat\'{e}rn kernel to infinity, 
we recover the exponential squared kernel.

It is possible to generalize the derivation of derivatives by using numerical
differentiation. Numerical differentiation may allow evaluation of other
kernels that generates isotropic and homogeneous data, 
such as the Mat\'{e}rn kernels.
% An implementation via numerical differentiation may
% or may not be more computationally intensive to
% compute, depending on the required numerical precision (aka order of accuracy).
Differentiation technology such as automatic differentiation, 
that spreads factors of the derivatives over a tree like structure for
almost exact computation can be considered, but is outside the scope of this work.

\subsection{Computational performance of our method}
We implemented the covariance kernels in \ref{eq:kernel_derivatives1}
via {\sc C++}. Although the operation of computing and assembling the
derivatives are $O(\ngal^2)$, the large number of addition and 
multiplication operations can be slow.
Exploiting the symmetry of the covariance kernel can only provide both speed and
memory saving of a factor of 2.
The parallelized computation of each element of the kernel was achieved using 
the OpenMP library.  

The slowest step of the entire computation, however, is the inversion of the
covariance kernel matrix for the evaluation of the data fit, 
which is a $O(\ngal^3)$ operation. 
We made use of parallelized Cholesky decomposition via the Intel Math Kernel Library
to perform the inversion of the matrix and achieved 10 times speed up so the
computation and the evaluation of the likelihood of each set of GP parameter
take $< 10s$ for 1000 galaxies.

Rather than relying on manual intensive methods for parallelizing the
computation of the kernel methods, a more promising way is to approximate the
small elements in the covariance kernel structure as zero. With large amount of
sparsity, The inversion of the matrix can be sped up to [TODO]$O()$.
The relevant field of literature about how the approximations may affect the
final statistics is called covariance tapering. On the other hand,  
the Machine Learning community refer to the support for the approximate kernel
as induce point methods.


\section{Conclusion}
Here, we have derived and implemented a fully probabilistic method for representing 
the lensing observables. 

The potential of using a Gaussian Process for the inference of cosmological 
Gaussian Process (GP)parameters remains to be proved. 

\section{Acknowledgement}
This work makes use of the supercomputing resources at the NERSC.
