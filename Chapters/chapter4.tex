 
This work on using Gaussian Process for cosmic shear analysis 
was first discussed as a possibility in  \citep{Schneider2014}  and 
forms the basis of the analyses performed in Schneider et al. (in prep.).

\section{Introduction} 
Dark matter (DM) forms an invisible cosmic web that can only be
probed by the influence of DM on visible galaxy tracers. 
This cosmic web encodes valuable information for constraining cosmological
parameters such as the matter density ($\Omega_m$) and the fluctuation amplitude 
of the matter density ($\sigma_8$) on a particular characteristic scale (8
$h^{-1} Mpc$). 
As demonstrated in chapter 1 of this work, we have shown that gravitational
lensing physics can relate the mass and spatial distribution of DM to the distortions
of galaxy shapes.  
The large scale lensing information that is used to constrain cosmological 
parameters is also known as cosmic shear. 
Here we discuss how to take into account other contributions to the galaxy shapes
to isolate the cosmic shear signal. 
This requires a multi-step and statistical approach with the use of many 
background galaxy tracers. We
illustrate how and why we may improve the representation of cosmic shear
information with the use of a Gaussian Process (GP). 


% signal - E mode 


% what are the noise - B mode ? 
First we Noisy process 
The inference starts from the measurement of the galaxy shapes [TODO: cite] as different
ellipticities components. Then various corrections are usually applied to 
account for the telescope systematics, exposure stacking and masking effects 
(\citealt{Rowe2010}, \citealt{Jee2013a}). For ground-based sky surveys,   
atmospheric turbulence also causes aberrations of the point-spread functions
(PSF). On the other hand, the ellipticities of galaxies have intrinsic
variance $\sigma_e^2$ without any lensing.    


star-galaxy separation, photometric redshift uncertainty  

% how do people summarize cosmic shear before
The large scale ($> 10 h^{-1} Mpc$) matter perturbation of the universe can be  
described by a Gaussian random field [TODO cite].  




% simulation to account for masking effects 


We depict with a probabilistic graphical model to illustrate how our work
fits into this process.
In terms of summarizing the DM lensing signal, past attempts   

 
including the intrinsic ellipticities ($\alpha$),
the noise from 

Statistical !!!!
There is a non-trivial, multi-step procedure 

% How to recover the Gaussian part of the primordial fluctuation  
% Gravitational lensing cosmic shear  
Cosmic shear inference is a non-trivial, multi-step process. It invo 
It requires a large number of galaxies for beating down noise and systematics. 
We need to know the signal as well as the noise. 



Past inference techniques that use statistical estimators for summarizing the 
density fluctuation of this cosmic web, such as  
the two-point correlation function (2PCF), or a closely related quantity from a
Fourier transform, the power spectrum, have shown promise to provide 
independent constraints \citep{Jee2013a}.  

careful background galaxy selection from contaminants
such as foreground stars, telescope systematics such as stacking exposure, 
survey masking, PSF correction 


%%% 
\cite{Schneider2014} proposed a flexible, Bayesian model that enables the
marginalization of nuisance parameters to allow for potentially less biased 
estimates of the cosmological parameters. 
We propose a flexible, generative regression model    
that can specify the joint distribution of the input variables while 
simultaneously capturing the rich physical relationships between the different
lensing observables.  



marginalize nuisance parameter 


In this work, we 
1) illustrate the basic properties of the Gaussian Process and how it is
similar or different from other approaches for cosmic shear inference  \\ 
2) show the derivation for incorporating the lensing physics in 
a suitable covariance kernel form for a Gaussian Process  \\
3) lay out the statistical model for making several mass maps 
with the Gaussian Process with the modified kernel and show preliminary results \\ 
4) discuss possible extensions and future directions 
 
Our method provides an alternative from having to use the 2PCF to represent the
Gaussian portion of the cosmic shear information. 
Source galaxies provide sparse constraints to probe the matter density along
the line of sight. 

\section{Method}

\subsection{The basics of a Gaussian Process}
A Gaussian Process is one of the most highly published and studied 
generative model. It allows the inference of the joint probability density 
function (PDF) for describing a set of data. 
This gives the model the flexibility for a wide range
of successful applications in many fields. 
In Astronomy, a GP has been used for modeling 
the light curve of exoplanets from discrete data points\citep{Ambikasaran2014a}.
Or the GP can be used as a prior probability for optimizing the model parameters of
neural networks \citep{Snoek2012}. 
It has also been used for other classification and pattern extraction tasks 
(\citealt{Wilson2013}, \citealt{Duvenaud2013}, \citealt{Rasmussen2006}).
We focus our discussion here on how the GP can be utilized to compute a prior
probability for cosmic shear inference.
 
It is helpful to understand the mathematical formulation of a GP 
before discussing how we can adapt the GP to model the lensing observables. 
In a nutshell, a GP smooths the input data using a non-parametric kernel
\citep{Hastie1990}. 
It is the generalization of a multivariate Gaussian 
to infinite dimension \citep{Rasmussen2006}. 
While a multivariate Gaussian is parametrized by  
the mean vector $\mathbf{\mu}$ and the covariance matrix $\Sigma$, 
a GP is specified by a mean vector function $m(\xv)$ and a
covariance kernel function $\kerngp(\xv, \yv)$, based on some input spatial or
temporal coordinates $\xv$. In our case, the vector $\xv$ (and the
 notation $\yv$ that is used interchangeably) denotes the
two-dimensional spatial coordinates of $\ngal$ source galaxy locations. 
The drawn collections of values $\psi(\xv)$ from a GP carry the correlation structures
specified by the kernel $\kerngp(\xv, \yv)$, and can be thought of as
functions: 
\begin{equation}
		[\psi_1(\xv), \psi_2(\xv) \ldots, \psi_m(\xv) ]^T \sim \gp(m(\xv), \kerngp(\xv, \xv')).
		\label{eq:GP}
\end{equation}
% Specifically, the mean vector $m(x)$ is often set to be zero in the
% GP. The data are usually mean-subtracted before fitting a GP.  
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{Gaussian_Process_Regression.png}
	\caption{An illustration of a one-dimensional GP. The
		function used to generate the data points 
		is shown with the dashed line. The available data points are shown in
		black. The mean interpolated prediction is shown in blue
		along with the gray region showing the 68\% 
		credible interval. This figure is taken from Cdipaolo96 at 
		the Wikimedia Commons under the Creative Commons license 4.0
\label{fig:one_d_gaussian_process}}
\end{figure}
This shows the probabilistic nature of the GP predictions
(See Fig. \ref{fig:one_d_gaussian_process}).
Each drawn function represents one realization of the smoothed
field, which can be used to model the lensing potential $\psi$ later on. 
By drawing many realizations of $\psi$, we can obtain the
corresponding credible levels.
Note that the credible levels are wider for regions with less data to reflect
higher uncertainty. A GP is said to have indefinite dimension due to its ability to
predict values in unobserved regions, even though the 
observed part of the kernel is of finite size (e.g. $\ngal \times \ngal$). 
By averaging the drawn realizations, we can
obtain the mean prediction. 


\subsection{Making predictions with a GP}
The mean predictions from a GP are
often more well behaved and do not show drastic increase or decrease 
in the tail regions where there is no data as other polynomial interpolation schemes 
would.  





When the input data is mean-subtracted, we can use a zero mean function in the
GP and the covariance kernel completely specifies the GP. 
There are several families of commonly used covariance kernel.
The ones that are of the highest interest to cosmic shear modeling 
are kernels that can capture 
the homogeneity and the isotropy of the data. This allows predicted spatial fields
$\psi$ to have consistent properties as the large-scale matter distribution.
%  Since our project wants to specify 
% The main difficulty of this project, 

The joint PDF can carry more information,
such as the uncertainties from the inputs, 
than summary statistics such as the 2PCF, peak statistics or .

A statistical model that makes use of the GP usually decomposes 
the signal and the noise in the following way: 
\begin{equation}
	e_{\rm obs} = g(\xv) + \epsilon_n^{\rm int}  
\end{equation}

% likelihood 
% prior 


\subsection{Adapting the exponential squared kernel with lensing physics}
One of the most popular kernels that can generate homogeneous and
isotropic data is the exponential squared kernel: 
\begin{equation}
	\kerngp(r^2) = \lambda^{-1} \exp\left(\frac{-r^2}{2 l^2}\right),
	\label{eq:exp_sq_kernel}
\end{equation}
where 
\begin{equation}
	r^2 \equiv (\xv - \yv)^T \Matrix{D} (\xv - \yv), 
\end{equation}
This kernel only depends on the
squared magnitude of distances between pairs of galaxy locations. 
This chosen form is therefore invariant
under translational and rotational transformations.
The metric $\Matrix{D}$ is taken to be a diagonal matrix in this discussion but  
can be generalized to account for anamorphic distortions. 
The precision parameter $\lambda$ affects the 
amplitude of the density perturbation, while the correlation length $l^2$ 
determines how fast the correlations between different spatial locations of the
field $\psi$ fall off as a function of the spatial distance.

Additionally, the exponential squared kernel is infinitely differentiable. This
differentiable kernel choice allows us to derive an analytical expression to
relate the different lensing observables.  
In the Newtonian limit, the scalar lensing potential $\psi$ is related to 
each of the lensing observables mentioned in chapter 1, e.g. 
$\lensparams \equiv [\kappa, \gamma_1, \gamma_2]$, via the following derivatives:
\begin{align}
\kappa &= \frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1^2} +
\frac{\partial^2 \psi}{\partial x_2^2 }\right) 
= \frac{1}{2} (\psi_{,11} + \psi_{,22}),\\ 
\gamma_1 
&=\frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1^2} - 
\frac{\partial^2 \psi}{\partial x_2^2}\right) 
= \frac{1}{2} (\psi_{,11} - \psi_{,22}), \\
\gamma_2 
&=\frac{1}{2}\left(\frac{\partial^2 \psi}{\partial x_1 \partial
x_2} + \frac{\partial^2 \psi}{\partial x_2 \partial x_1}\right)
= \frac{1}{2} (\psi_{,12} + \psi_{,21}), 
\end{align}
where we have defined the shorthand for spatial derivatives with
subscripts for $h,i,j,k = 1, 2$ after a comma.
Since the covariance operator is linear, the 4th derivatives of the covairance
kernels have the following forms: 
\begin{align}
	\label{eq:kernel_derivatives1}
	\Cov (\kappa(\xv), \kappa(\yv))
&= \frac{1}{4}\left(
\kerngp_{,1111} + \kerngp_{,1122} + \kerngp_{,2211} + \kerngp_{,2222}
\right), \\
\Cov(\kappa(\xv), \gamma_1(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1111} + \kerngp_{,2211} - \kerngp_{,1122} - \kerngp_{,2222}
\right), \\
\Cov(\kappa(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1112} + \kerngp_{,2212} + \kerngp_{,1121} + \kerngp_{,2221}
\right),\\
\Cov(\gamma_1(\xv), \gamma_1(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1111} - \kerngp_{,1122} - \kerngp_{,2211} + \kerngp_{,2222}
\right), \\
\Cov(\gamma_1(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1112} + \kerngp_{,1121} - \kerngp_{,2212} - \kerngp_{,2221}
\right), \\
\Cov(\gamma_2(\xv), \gamma_2(\yv)) &= \frac{1}{4}\left(
\kerngp_{,1212} + \kerngp_{,1221} + \kerngp_{,2112} + \kerngp_{,2121}
\right),
	\label{eq:kernel_derivatives2}
\end{align}
where
\begin{equation}
	\kerngp_{,hijk} = \frac{\partial^4 \kerngp}{\partial x_h \partial x_i
	\partial y_j \partial y_k}.
\end{equation}

With the definition of ${\bf X}_i$ as follows:
\begin{equation}
	\frac{\partial r^2}{\partial \xv_i} = -\frac{\partial
	r^2}{\partial \yv_i} =
	2 \Matrix{D}(\xv - \yv)_i \equiv 2{\bf X}_i,
\end{equation}
we can show that each entry $\nu_{,hijk}$ of $\kerngp_{,hijk}$ is
related to each entry $\nu$ of the original exponential squared kernel
$\kerngp$ by:
\begin{equation}
\nu_{,x_h x_i y_j y_k} = (\beta^4 X_h X_i X_j X_k -
\beta^3 (X_h X_i \Matrix{D}_{jk} \delta_{jk} + 5~{\rm perm.}) + \beta^2
(\Matrix{D}_{hj} \Matrix{D}_{ik}\delta_{hj}\delta_{ik} + 2~{\rm perm.})) \nu,
\label{eq:4thderivatives}
\end{equation}
with $\beta = l^{-2}$. There are 6 permutations (abbreviated as perm.) of the terms in
eq. \ref{eq:4thderivatives}
multiplied to $\beta^3$ due
to choosing two pairs of indices from the $h,i,j,k$ where the order of each
pair of indices matters when we differentiate the terms by parts. 
Likewise, there are 3 possible permutations of
the terms the multiplied to $\beta^2$ due to choosing two pairs of indices from
$h, i, j, k$ where the order of the pairs does not matter.
The derivation of the relevant derivatives and terms are available 
in the appendix for completeness. 
Our implementation of the GP kernels in eqn \ref{eq:kernel_derivatives1} - 
\ref{eq:kernel_derivatives2} is openly available at
\href{https://github.com/karenyyng/george}{https://github.com/karenyyng/george}.


\subsection{Bayesian framework for inferring cosmic shear}
\begin{figure}
	\centering
	\includegraphics[width=0.4\linewidth]{shear_gp_pgm.png}
	\caption{A simplified probabilistic graphical model (pgm) for inferring
		the lensing observables $\Upsilon$ from survey or simulation data. The dots
		denote fixed parameters in the statistical procedure. The circle denotes
		latent variable, while the shaded circle denotes 
		the observed variable. The plate notation shows the galaxy entries to sum
		over.
		\label{fig:simplified_pgm}}
\end{figure}

Throughout the analyses present in this work, 
we make use of the weak shear approximation 
$g = \gamma / (1 - \kappa)  \approx \gamma$, so the observed ellipticity can be
written as: 
\begin{equation}
	\epsilon_{\rm obs} = \epsilon_{\rm int} + g 
\end{equation}
assume that the shape noise term $\epsilon_{\rm int}$ of each galaxy is 
independent and follow the same Gaussian distribution.  

\subsection{The computational performance of our method}
We implemented the covariance kernels in \ref{eq:kernel_derivatives1}
via {\sc C++} along with a {\sc Python} wrapper. 
Although the operation of computing and assembling the
derivatives are $O(\ngal^2)$, the large number of addition and 
multiplication operations can be slow.
Exploiting the symmetry of the covariance kernel can only provide 
memory saving of a factor of 2.
To speed up the computation to make it practical, 
I implemented the parallelized computation of each element of the kernel using 
the {\sc OpenMP} library. The slowest step of the entire computation, however, 
is the inversion of the
covariance kernel matrix for the evaluation of the marginal likelihood, 
which is an $O(\ngal^3)$ operation. 
I made use of parallelized Cholesky decomposition via the {\sc Intel Math
Kernel Library} to perform the inversion of the matrix and achieved $\sim10$ 
times speed up so the
computation and the evaluation of the likelihood of each set of GP parameter
take $< 10s$ for 1600 galaxies, which translates to 
the inversion of a 4200 $\times$ 4200 covariance matrix.
With the use of distributed computational resources, it is possible 
to probe the space of possible GP parameters.  


We will discuss possible methods
of speed up in section [TODO].

\section{Results}
 
\begin{table}%[htb]
% \small
\begin{center}
\caption{Parameters for the statistical model.}
\label{tab:sampling_parameters}
\begin{tabular}{cl}
\hline
Parameter & Description \\
\hline
${\bf d_{n}}$ & data vector (measured $e_{1,2}$ for each galaxy $n$)  \\
$\sigma_{{\rm ms}; n}$ & ellipticity measurement error for galaxy $n$ 
\\
$\xv$, $\yv$ & vector(s) of 2D spatial locations of galaxies \\
$\epsilon_n^{\rm int}$ & intrinsic galaxy ellipticity for galaxy $n$ \\
$\alpha\equiv\sigma_{e}^2$ & parameters of the distribution of galaxy parameters \\
W & window function for the survey footprint \\
$\lensparams$ & lensing shear and convergence ($\gamma_{1,2}, \kappa$) \\
$\gpparams$ & parameters of the GP kernel\\
% $\rho_{\rm GP}$ & correlation of the GP kernel (element of $\gpparams$) \\
$\lambda$ & precision of the GP kernel (element of $\gpparams$) \\
$l^2$ & squared GP correlation length (element of $\gpparams$) \\
% $\lambda_{\epsilon}$ & precision for the nugget term (element of $\gpparams$) \\
\hline
\end{tabular}
\end{center}
\end{table}



\subsection{Verification using {\sc GalSim} simulated data}
\subsection{Mass mapping for Deep Lens Survey Abell 781}

% Is a non-trivial optimization problem 

\section{Discussion}

\subsection{Relating the lensing observables to a cosmological model}
We can draw realizations of the lensing observables from the GP given a set of
optimized GP parameters:
\begin{equation}
	\lensparams(\xv) \sim GP(0, \kerngp_{\rm GP}(\xv, \yv)) 
\end{equation}


The constraints on cosmological parameters need to be verified in a mock
weak-lensing analysis of cosmological simulation. 
It can enable the separation of E and B mode in the convergence field as shown
in Schneider et al. (in prep.) 

\subsection{Use of the Gaussian Process in the model fitting for shear}
We describe a simplified graphical model to demonstrate how the derived GP can
be used to infer cosmic shear and a mass map from the convergence.
A more sophisticated model can be found in Schneider et al. 2016.



% - [ ] TODO add PGM 
% talk about the lensing approximations that were made 


% \subsection{Sampling grid and the correlation length}
% A weakness of the method is the apparent reliance on having the sampling 

\subsection{Alternative kernel choice(s) for encoding the lensing physics}
Alternative choices for a covariance kernel for describing the lensing physics 
include the family of Mat\'{e}rn kernels:
% \begin{equation}
% 
% \end{equation}

The smoothness of the data generated from a Mat\'{e}rn kernel is closely
related to the degree of freedom of the kernel. 
The higher degree of freedom it has, the smoother the spatial variations of the
drawn $\psi$ would be for the same correlation length.
Note that a Mat\'{e}rn kernel is also only differentiable to the $i$-th order,
where $i$ is [TODO] related to the degree of freedom by [TODO].
This restricts us to use kernels with degree of freedom above [TODO]. 
When we take the limit of the degree of freedom of a Mat\'{e}rn kernel to infinity, 
we recover the exponential squared kernel.

It is possible to generalize the derivation of derivatives by using automatic
differentiation. Automatic differentiation may allow evaluation of other
kernels that generates isotropic and homogeneous data, 
such as the Mat\'{e}rn kernels.  
% An implementation via numerical differentiation may
% or may not be more computationally intensive to
% compute, depending on the required numerical precision (aka order of accuracy).
Differentiation technology such as automatic differentiation, 
that spreads factors of the derivatives over a tree like structure for
almost exact computation can be considered, but is outside the scope of this work.



\subsection{Possible separation of the E / B mode}

% - [ ] describe why the lensing observables shouldn't have a B mode 

It is not obvious what kernel choices will allow one to capture the B-mode 
in the data.
We can separate the E and the B mode of the shear, those derivation can be seen in  
\cite{Schneider2001a}.
 \begin{align}
	\kappa^E & = \kappa\\
	\kappa^B & = 0 \\
	\gamma^E &= \left[\frac{1}{2} (\psi^E_{,11} - \psi^E_{,22}) -
	\psi^B_{,12}\right]\\
	\gamma^B &= \left[\psi^E_{,12} + \frac{1}{2} (\psi^B_{,11} - \psi^B_{,22})\right]
\end{align}


\subsection{Modeling systematics and noise}
Besides kernels that describes homogeneous and isotropic data,
there are other types of periodic kernels that may be useful
in modeling systematics such as masking or footprints of chip gaps due to the
stacking of multiple exposures.
For spatial models with undetermined periodicity or patterns, 
composite kernels with different covariance structures 
can be formed by the addition and/or multiplication of several kernels
 to pick up (sometimes non-obvious) patterns in the data. 
A GP with composite kernels is a
famous example how both the short-and-long term trends of global carbon
dioxide level can be modeled and predicted \citep{Duvenaud2013}.



% Can handle sharp boundaries from stacking images 
% cite Jee2013a  
% cite Rowe2010 paper for interpolating star fields for 
% PSF ellipticity corrections 

\subsection{Speeding up the matrix operations}
Currently, our method is not as computationally competitive as the 2PCF. One of
the fastest implementation has a computational complexity of $O(N\sqrt{N})$.
The 2PCF algorithm has also been demonstrated to scale well with a distributed and parallelized
implementation \citep{Chhugani2012}.

Rather than relying on manual intensive methods for parallelizing the
computation of the kernel methods, a more promising way is to approximate the
small elements in the covariance kernel structure as zero. 

The relevant field of literature about how approximating the kernel as a sparse
matrix may affect the
statistical inference is called covariance tapering. 
On the other hand, \citep{Snelson2006}
there are papers demonstrating that synthetic data with much less data points  
would produce further 
With large amount of
sparsity, the inversion of the matrix can be sped up to [TODO]$O(M^2N)$ for
building the model and $O(M^2)$ for interpolating / predicting output functions
at new input locations \citep{Snelson2006}.
  

\section{Conclusion}
Here, we have derived and implemented a fully probabilistic method for 
representing the lensing observables. It can potentially  

The potential of using a Gaussian Process for the inference of cosmological 
Gaussian Process (GP)parameters remains to be proved. 

Requires the understanding of the underlying physical observable, how the statistical
approach can carry the information and how to actual execute the computation in
a tractable way.


\section{Acknowledgement}
We thank Chris Paciorek for discussion about the statistical
framework for performing shear inference and the
use of Gaussian Processes.
This research
used the resources of the National Energy Research Scientific 
Computing Center (NERSC), a DOE Office of Science
User Facility supported by 
of the U.S. Department of Energy under Contract No.
DE-AC02-05CH11231.
 This work uses a modified version
of the public code {\sc George} available at \href{https://
github.com/karenyyng/george}{https://
github.com/karenyyng/george}, which was forked from \\
\href{https://github.com/dfm/george}{https://github.com/dfm/george}.



\appendix 

\section{Derivation of GP kernel}
\begin{equation*}
\Sigma = \lambda^{-1} \kerngp 
\end{equation*}

\begin{align}
\kerngp &= \exp{\left(\frac{-\beta}{2} r^2 \right)}\\
\kerngp_{,x_i} &= \frac{-\beta}{2} \kerngp r^2_{,x_i} = -\beta \kerngp X_i\\ 
\kerngp_{,y_h} &= \beta \kerngp X_h 
\end{align}

\begin{align}
\kerngp_{,x_i x_j} &= \frac{-\beta}{2}(\kerngp_{,x_j} r^2_{,x_i} + \kerngp r^2_{,x_i x_j})\\
\kerngp_{,x_i x_j y_h} &= \frac{-\beta}{2}(\kerngp_{,x_j y_h} r^2_{,x_i} + \kerngp_{,x_j}
r^2_{,x_i y_h} + \kerngp_{,y_h} r^2_{,x_i x_j})\\
\kerngp_{,x_i x_j y_h y_k} &= \frac{-\beta}{2} 
(\kerngp_{,x_j y_h y_k} r^2_{,x_i} +
\kerngp_{,x_j y_h} r^2_{,x_i y_k} + 
\kerngp_{,x_j y_k} r^2_{,x_i y_h} + 
\kerngp_{,y_h y_k} r^2_{,x_i x_j} )
\label{eq:4th_derivative_intermediate_expression}
\end{align}

Just work on terms that are parts of the 4th kernel derivative 
\begin{align*}
\kerngp_{,x_i x_j} &= \frac{\partial}{\partial x_j} (-\beta \kerngp X_i)\\
&= -\beta(\kerngp_{,x_j X_i} + \kerngp X_{i, x_j})\\ 
&= -\beta(-\beta \kerngp X_j X_i + \kerngp \delta_{ij} \mathsf{D}_{ij}) \\ 
&= (\beta^2 X_j X_i - \beta \delta_{ij} \mathsf{D}_{ij})\kerngp 
\end{align*}

\begin{align*}
\kerngp_{,x_i y_h} &= \frac{\partial}{\partial y_h} (-\beta \kerngp X_i)\\
&= -\beta(\kerngp_{,y_h X_i} + \kerngp X_{i, y_h})\\ 
&= -\beta(\beta \kerngp X_h X_i - \kerngp \delta_{ih} \mathsf{D}_{ih}) \\ 
&= -(\beta^2 X_h X_i - \beta \delta_{ih} \mathsf{D}_{ih})\kerngp 
\end{align*}

\begin{align*}
\kerngp_{,y_h y_k} &= \frac{\partial}{\partial y_h} (\beta \kerngp X_h)\\
&= \beta(\kerngp_{,y_k X_h} + \kerngp X_{h, y_k})\\ 
&= \beta(\beta \kerngp X_k X_h - \kerngp \delta_{hk} \mathsf{D}_{hk}) \\ 
&= (\beta^2 X_h X_k - \beta \delta_{hk} \mathsf{D}_{hk})\kerngp 
\end{align*}

\begin{align}
\kerngp_{,x_i x_j} &= (\beta^2 X_j X_i - \beta \delta_{ij} \mathsf{D}_{ij})\kerngp\\ 
\kerngp_{,x_i y_h} &= -(\beta^2 X_h X_i - \beta \delta_{ih} \mathsf{D}_{ih})\kerngp\\ 
\kerngp_{,y_h y_k} &= (\beta^2 X_h X_k - \beta \delta_{hk} \mathsf{D}_{hk})\kerngp 
\end{align}

Term 1 of the 4th derivative of eqn \ref{eq:4th_derivative_intermediate_expression}

\begin{align*}
\kerngp_{,x_j y_h y_k}
&= \frac{\partial}{\partial y_k} \kerngp_{,x_j y_h}\\ 
&= -\frac{\partial}{\partial y_k} (\beta^2 X_h X_j - \beta \delta_{jh} \mathsf{D}_{jh})\kerngp\\
&= (\beta^2 \mathsf{D}_{hk} \delta_{hk} X_j + \beta^2 X_h \mathsf{D}_{jk}\delta_{jk})\kerngp -
(\beta^2 X_h X_j - \beta \mathsf{D}_{jh} \delta_{jh})\beta X_k \kerngp
\end{align*}

\begin{align*}
&\kerngp_{,x_j y_h y_k} r_{,x_i}^2\\
&= 2[\beta^2 X_j \mathsf{D}_{hk} \delta_{hk} + \beta^2 X_h \mathsf{D}_{jk}\delta_{jk} +
\beta^2 X_k \mathsf{D}_{jh} \delta_{jh} - \beta^3 X_h X_j X_k] X_i \kerngp\\ 
&=\boxed{2\beta^2[ 
X_j X_i \mathsf{D}_{hk} \delta_{hk} + 
X_h X_i \mathsf{D}_{jk} \delta_{jk} + 
X_k X_i \mathsf{D}_{jh} \delta_{jh}] \kerngp 
- 2\beta^3 X_h X_j X_k X_i \kerngp} 
\end{align*}

\subsection{Term 2 of the 4th derivative}
\begin{align*}
\kerngp_{,x_j y_h} r^2_{,x_i y_k}
&= - (\beta^2 X_h X_j - \beta \mathsf{D}_{jh} \delta_{jh}) (-2\mathsf{D}_{ik}
\delta_{ik} \kerngp) \\  
&= \boxed{( 2 \beta^2  X_h X_j \mathsf{D}_{ik} \delta_{ik} 
-2 \beta \mathsf{D}_{jh} \mathsf{D}_{ik} \delta_{jh} \delta_{ik}) \kerngp} 
\end{align*}

\subsection{Term 3 of the 4th derivative}
This is completely analogous to term 2 except the subscripts are slightly
different
\begin{align*}
\kerngp_{,x_j y_k} r^2_{,x_i y_h}
&=\boxed { ( 2 \beta^2  X_k X_j \mathsf{D}_{ih} \delta_{ih} 
-2 \beta \mathsf{D}_{jk} \mathsf{D}_{ih} \delta_{jk} \delta_{ih}) \kerngp} 
\end{align*}

\subsection{Term 4 of the 4th derivative} 
\begin{align*}
\kerngp_{,y_h y_k} r^2_{,x_i x_j}
&= (\beta^2 X_k X_h - \beta \mathsf{D}_{hk}\delta_{hk})\kerngp 2 \mathsf{D}_{ij} \delta_{ij}\\ 
&= \boxed{(2\beta^2 X_k X_h \mathsf{D}_{ij} \delta_{ij} - 2\beta \mathsf{D}_{ij} \mathsf{D}_{hk} \delta_{hk}
\delta_{ij})\kerngp } 
\end{align*}

Collect terms of $\Sigma_{,x_i x_j y_h y_k}$ by plugging them in eqn
\ref{eq:4th_derivative_intermediate_expression}
from the relevant terms that are boxed above, 
\begin{align}
\nu_{,x_i x_j y_h y_k} &= (\beta^4 X_h X_j X_k X_i -  
\beta^3 (X_j X_i \mathsf{D}_{hk} \delta_{hk} + 5 {\rm perm.}) + \beta^2
(\mathsf{D}_{jh} \mathsf{D}_{ik}\delta_{jh}\delta_{ik} + 2 {\rm perm.})) \nu \\
&= \gamma \nu
\end{align}
where $\nu$ is an entry in the exponential squared kernel matrix $\kerngp$.
